{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a71a7b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import torch\n",
    "import import_ipynb\n",
    "import import_ipynb\n",
    "from localAngle import calc_localAngle\n",
    "from coordinate import coordinateFeature\n",
    "from coordinate import normFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0abfe328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Local Angle For Node Feature\n",
    "def local_angles(BINARY_ADJACENCY_MATRIX, x_axis, y_axis, z_axis):\n",
    "    row_indices, col_indices = BINARY_ADJACENCY_MATRIX.nonzero()\n",
    "    Local_Angle_features=[]\n",
    "    for landmark in range(468):\n",
    "        List = []\n",
    "        for i in range(len(row_indices)):\n",
    "            if(row_indices[i] == landmark):\n",
    "                List.append([x_axis[col_indices[i]], y_axis[col_indices[i]], z_axis[col_indices[i]]])\n",
    "        A = [x_axis[landmark], y_axis[landmark], z_axis[landmark]]\n",
    "        # print(\"A=\",A, \"S=\",List)\n",
    "        Local_Angle_features.append(calc_localAngle(A, np.array(List)))\n",
    "    # print(\"Local_Angle_features=\",Local_Angle_features)\n",
    "    return normFunction(Local_Angle_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42397729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe(path): \n",
    "    image=cv2.imread(path)\n",
    "    image=cv2.resize(image,(600,600))\n",
    "    annoted_image=image.astype(np.uint8)\n",
    "\n",
    "    mp_face_mesh=mp.solutions.face_mesh\n",
    "    connection_tesselation=mp_face_mesh.FACEMESH_TESSELATION\n",
    "    # print(\"edges_mediapipe=\",len(connection_tesselation),len(connection_tesselation)/2)\n",
    "    # print(connection_tesselation)\n",
    "    with mp_face_mesh.FaceMesh(static_image_mode=False,max_num_faces=2,refine_landmarks=True,min_detection_confidence=0.5) as face_mesh:\n",
    "        results=face_mesh.process(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "\n",
    "    x_axis=np.empty(len(results.multi_face_landmarks[0].landmark))\n",
    "    y_axis=np.empty(len(results.multi_face_landmarks[0].landmark))\n",
    "    z_axis=np.empty(len(results.multi_face_landmarks[0].landmark))\n",
    "    i=0\n",
    "\n",
    "    for nodes in results.multi_face_landmarks[0].landmark:\n",
    "        x_axis[i]=(results.multi_face_landmarks[0].landmark[i].x)\n",
    "        y_axis[i]=(results.multi_face_landmarks[0].landmark[i].y)\n",
    "        z_axis[i]=(results.multi_face_landmarks[0].landmark[i].z)\n",
    "        i=i+1\n",
    "    # print(len(results.multi_face_landmarks[0].landmark))\n",
    "    # print(\"xyz=\",x_axis)\n",
    "\n",
    "    FEATURE_MATRIX=np.zeros((468,468))\n",
    "    for i in range(468):\n",
    "        FEATURE_MATRIX[i][i]=1\n",
    "\n",
    "    WEIGHTED_ADJACENCY_MATRIX=np.zeros((468,468)) #creating a numpy array of shape 468X468 initialized with zero\n",
    "    BINARY_ADJACENCY_MATRIX=np.zeros((468,468))\n",
    "\n",
    "    for edge in connection_tesselation:\n",
    "        # Determining X & Y axis of Two Connected Points\n",
    "        x1=x_axis[edge[0]]\n",
    "        y1=y_axis[edge[0]]\n",
    "        z1=z_axis[edge[0]]\n",
    "        x2=x_axis[edge[1]]\n",
    "        y2=y_axis[edge[1]]\n",
    "        z2=z_axis[edge[1]]\n",
    "        # Calculating Eucleadin Distance for Weighted Graph\n",
    "        eucleadian_distance=(((x2-x1)**2 + (y2-y1)**2 + (z2-z1)**2) ** 0.5)\n",
    "        # Creating Weighted Graph\n",
    "        WEIGHTED_ADJACENCY_MATRIX[edge[0]][edge[1]]=eucleadian_distance\n",
    "        WEIGHTED_ADJACENCY_MATRIX[edge[1]][edge[0]]=eucleadian_distance\n",
    "\n",
    "        BINARY_ADJACENCY_MATRIX[edge[0]][edge[1]]=1\n",
    "        BINARY_ADJACENCY_MATRIX[edge[1]][edge[0]]=1\n",
    "        # Normalizing axis Values For Visualizing Meash on Face\n",
    "        shape = image.shape \n",
    "        relative_x = int(x1 * shape[1])\n",
    "        relative_y = int(y1 * shape[0])\n",
    "        relative_z = int(x2 * shape[1])\n",
    "        relative_c = int(y2 * shape[0])\n",
    "        # Drawing Nodes and Edges on Image For Visualization\n",
    "        cv2.circle(image, (relative_x, relative_y), radius=1, color=(0, 0, 255), thickness=3) \n",
    "        cv2.line(image,(relative_x,relative_y),(relative_z,relative_c),(0,255,0),1)\n",
    "\n",
    "    # plt.imshow(image)\n",
    "    WEIGHTED_ADJACENCY_MATRIX=sparse.csr_matrix(WEIGHTED_ADJACENCY_MATRIX)\n",
    "    BINARY_ADJACENCY_MATRIX=sparse.csr_matrix(BINARY_ADJACENCY_MATRIX)\n",
    "    # print(BINARY_ADJACENCY_MATRIX)\n",
    "    LocalAngles = local_angles(BINARY_ADJACENCY_MATRIX, x_axis, y_axis, z_axis)\n",
    "    # print(LocalAngles)\n",
    "    XCoordinateFeature, YCoordinateFeature, ZCoordinateFeature = coordinateFeature(x_axis, y_axis, z_axis)\n",
    "    # FEATURE_MATRIX=torch.from_numpy(FEATURE_MATRIX)\n",
    "    DOTPRODUCT=WEIGHTED_ADJACENCY_MATRIX.dot(BINARY_ADJACENCY_MATRIX)\n",
    "    # newFeatureMatrix = LocalAngles\n",
    "    newFeatureMatrix = torch.from_numpy(np.column_stack((LocalAngles,  XCoordinateFeature[:-10], YCoordinateFeature[:-10], ZCoordinateFeature[:-10], FEATURE_MATRIX)))\n",
    "    # print( LocalAngles[0], XCoordinateFeature[:-10][0], YCoordinateFeature[:-10][0], ZCoordinateFeature[:-10][0])\n",
    "    return(WEIGHTED_ADJACENCY_MATRIX,newFeatureMatrix)\n",
    "    # return(DOTPRODUCT,newFeatureMatrix/np.linalg.norm(newFeatureMatrix))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "624e45dc",
   "metadata": {},
   "source": [
    "Checking mediapipe is running currectly or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81b2d45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.7759, -0.0235,  0.5058, -0.4985], dtype=torch.float64)\n",
      "tensor([-0.5424,  0.0069,  0.3426, -0.9304], dtype=torch.float64)\n",
      "tensor([ 0.6072, -0.0030,  0.3774, -0.5853], dtype=torch.float64)\n",
      "tensor([-0.4404, -0.0540,  0.0469, -0.8909], dtype=torch.float64)\n",
      "tensor([-0.6990,  0.0128,  0.2676, -1.0000], dtype=torch.float64)\n",
      "tensor([-0.7419,  0.0193,  0.1480, -0.9965], dtype=torch.float64)\n",
      "tensor([-0.1445,  0.0319, -0.1702, -0.8136], dtype=torch.float64)\n",
      "tensor([-0.2440, -0.5995, -0.2972, -0.3943], dtype=torch.float64)\n",
      "tensor([ 0.0725,  0.0438, -0.3957, -0.8246], dtype=torch.float64)\n",
      "tensor([ 0.0010,  0.0507, -0.5089, -0.9042], dtype=torch.float64)\n",
      "tensor([-0.4583,  0.0676, -0.9941, -0.9860], dtype=torch.float64)\n",
      "tensor([-0.6409, -0.0252,  0.5234, -0.4675], dtype=torch.float64)\n",
      "tensor([-0.6022, -0.0280,  0.5377, -0.4196], dtype=torch.float64)\n",
      "tensor([-0.7103, -0.0302,  0.5464, -0.3439], dtype=torch.float64)\n",
      "tensor([-0.6592, -0.0333,  0.5456, -0.3258], dtype=torch.float64)\n",
      "tensor([-0.1245, -0.0347,  0.5819, -0.3144], dtype=torch.float64)\n",
      "tensor([-0.3159, -0.0367,  0.6229, -0.3224], dtype=torch.float64)\n",
      "tensor([-0.1918, -0.0394,  0.6611, -0.2866], dtype=torch.float64)\n",
      "tensor([-0.2710, -0.0435,  0.7649, -0.1490], dtype=torch.float64)\n",
      "tensor([ 0.7616,  0.0033,  0.3681, -0.8551], dtype=torch.float64)\n",
      "tensor([ 0.0339, -0.0876,  0.3438, -0.6916], dtype=torch.float64)\n",
      "tensor([-0.5352, -0.9426, -0.7206, -0.0401], dtype=torch.float64)\n",
      "tensor([-0.3225, -0.3365, -0.2066, -0.4826], dtype=torch.float64)\n",
      "tensor([ 0.1100, -0.4293, -0.2012, -0.4740], dtype=torch.float64)\n",
      "tensor([-0.1482, -0.5194, -0.2110, -0.4453], dtype=torch.float64)\n",
      "tensor([-0.5621, -0.6332, -0.2791, -0.3624], dtype=torch.float64)\n",
      "tensor([ 0.1622, -0.2586, -0.2240, -0.4719], dtype=torch.float64)\n",
      "tensor([ 0.6220, -0.4929, -0.4425, -0.5978], dtype=torch.float64)\n",
      "tensor([-0.2164, -0.3839, -0.4298, -0.5980], dtype=torch.float64)\n",
      "tensor([-0.4319, -0.5864, -0.4320, -0.5543], dtype=torch.float64)\n",
      "tensor([ 0.1174, -0.6427, -0.4062, -0.5005], dtype=torch.float64)\n",
      "tensor([-0.4146, -0.7094, -0.2394, -0.2854], dtype=torch.float64)\n",
      "tensor([-0.5664, -0.3556,  0.8229,  0.0493], dtype=torch.float64)\n",
      "tensor([ 0.0448, -0.6325, -0.3307, -0.3643], dtype=torch.float64)\n",
      "tensor([-0.2521, -0.9829, -0.3585,  0.1966], dtype=torch.float64)\n",
      "tensor([-1.0000, -0.8168, -0.3196, -0.1994], dtype=torch.float64)\n",
      "tensor([-0.6546, -0.4541,  0.1416, -0.4260], dtype=torch.float64)\n",
      "tensor([-0.0186, -0.1372,  0.4749, -0.4933], dtype=torch.float64)\n",
      "tensor([-0.6002, -0.1311,  0.5186, -0.4069], dtype=torch.float64)\n",
      "tensor([-0.0238, -0.2367,  0.4628, -0.4193], dtype=torch.float64)\n",
      "tensor([-0.5564, -0.3082,  0.4511, -0.3209], dtype=torch.float64)\n",
      "tensor([-0.5424, -0.2175,  0.4967, -0.3622], dtype=torch.float64)\n",
      "tensor([-0.9762, -0.2868,  0.4771, -0.2768], dtype=torch.float64)\n",
      "tensor([ 0.0077, -0.4406,  0.5672, -0.0644], dtype=torch.float64)\n",
      "tensor([-0.1748, -0.0669,  0.3342, -0.9236], dtype=torch.float64)\n",
      "tensor([-0.0921, -0.0733,  0.2601, -0.9895], dtype=torch.float64)\n",
      "tensor([ 0.2251, -0.7735, -0.4800, -0.5165], dtype=torch.float64)\n",
      "tensor([-0.1058, -0.2551, -0.0444, -0.5314], dtype=torch.float64)\n",
      "tensor([-0.6423, -0.2827,  0.2445, -0.6512], dtype=torch.float64)\n",
      "tensor([ 0.1019, -0.2850,  0.1924, -0.6320], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# path=\"./datasets/orl_dataset/person10/test_image/98_10.jpg\"\n",
    "# adj1,features1=mediapipe(path)\n",
    "# for i in range(50):\n",
    "#     print(features1[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5776e122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj1=adj1.toarray()\n",
    "# adj2=adj2.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfe064a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path=\"./datasets/orl_dataset/person9/test_image/88_9.jpg\"\n",
    "# adj1,features1=mediapipe(path)\n",
    "# for i in range(50):\n",
    "#     print(features1[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f86ed6d2ccfca78b79da216d0cb38b107327236eab96f22d38fd0505e1d75fcf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
